use ndarray::{Array, Array1};
use ndarray_rand::RandomExt;
use rand_distr::Uniform;

use lumen::autograd::{no_grad, Tensor};
use lumen::loader::ModelLoader;
use lumen::models::{LlamaConfig, LlamaModel};
use lumen::tokenizer::LlamaTokenizer;

use std::io::{self, Write};
use mimalloc::MiMalloc;

#[global_allocator]
static GLOBAL: MiMalloc = MiMalloc;

// æ„å»º Promptï¼ˆTinyLlama chat templateï¼‰
fn build_tinyllama_chat_prompt(system: &str, user: &str) -> String {
    format!(
        "<|system|>\n{}\n</s>\n<|user|>\n{}\n</s>\n<|assistant|>\n",
        system, user
    )
}

// å›é€€åˆ°åˆæ³• UTF-8 è¾¹ç•Œï¼ˆç”¨äºæµå¼è¾“å‡ºï¼‰
fn lcp_char_boundary(prev: &str, cur: &str) -> usize {
    let pb = prev.as_bytes();
    let cb = cur.as_bytes();
    let mut i = 0usize;
    let n = pb.len().min(cb.len());
    while i < n && pb[i] == cb[i] {
        i += 1;
    }
    while i > 0 && !cur.is_char_boundary(i) {
        i -= 1;
    }
    i
}

fn print_new_suffix(prev_printed: &mut String, cur_full: String) {
    if cur_full.contains('\u{FFFD}') {
        return;
    }
    let cut = lcp_char_boundary(prev_printed, &cur_full);
    if cut < cur_full.len() {
        print!("{}", &cur_full[cut..]);
        let _ = io::stdout().flush();
    }
    *prev_printed = cur_full;
}

// é‡‡æ ·å‡½æ•°ï¼šé¿å…ä½¿ç”¨ rng.genï¼ˆåœ¨ Rust 2024 é‡Œ gen æ˜¯ä¿ç•™å­—ï¼‰
#[inline]
fn rand01() -> f32 {
    Array1::<f32>::random(1, Uniform::new(0.0f32, 1.0f32))[0]
}

fn sample_top_p(
    logits: &[f32],
    temperature: f32,
    top_p: f32,
    repetition_penalty: f32,
    recent_tokens: &[usize],
) -> usize {
    // 1) Repetition Penalty
    let mut adjusted = logits.to_vec();
    if repetition_penalty > 1.0 {
        for &t in recent_tokens {
            if t < adjusted.len() {
                adjusted[t] /= repetition_penalty;
            }
        }
    }

    // 2) Temperature
    let temp = temperature.max(1e-5);
    for v in adjusted.iter_mut() {
        *v /= temp;
    }

    // 3) Softmax
    let mut maxv = f32::NEG_INFINITY;
    for &v in &adjusted {
        if v > maxv {
            maxv = v;
        }
    }
    let mut probs: Vec<f32> = adjusted.iter().map(|x| (x - maxv).exp()).collect();
    let sum: f32 = probs.iter().sum();
    let inv = 1.0 / (sum + 1e-9);
    for p in probs.iter_mut() {
        *p *= inv;
    }

    // 4) Top-P
    let mut idxs: Vec<usize> = (0..probs.len()).collect();
    idxs.sort_by(|&i, &j| probs[j].partial_cmp(&probs[i]).unwrap());

    let mut cumulative = 0.0f32;
    let mut cut = 0usize;
    for (rank, &i) in idxs.iter().enumerate() {
        cumulative += probs[i];
        cut = rank + 1;
        if cumulative >= top_p {
            break;
        }
    }
    idxs.truncate(cut.max(1));

    // 5) Sample
    let r = rand01();
    let mut acc = 0.0f32;
    for &i in &idxs {
        acc += probs[i] / cumulative;
        if r <= acc {
            return i;
        }
    }
    *idxs.last().unwrap()
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆå§‹åŒ–é…ç½®ï¼ˆTinyLlama-1.1Bï¼Œéœ€ä¸ä½ çš„æƒé‡ä¸€è‡´ï¼‰
    let config = LlamaConfig {
        vocab_size: 32000,
        hidden_size: 2048,
        intermediate_size: 5632,
        num_hidden_layers: 22,
        num_attention_heads: 32,
        rms_norm_eps: 1e-5,
        max_seq_len: 2048,
        num_key_value_heads: 4, // GQA
        rope_theta: 10000.0,
    };

    println!("ğŸ¦€ Loading Rusty Llama...");
    let model = LlamaModel::new(config.clone());

    // TODO: æ”¹æˆä½ æœ¬æœºè·¯å¾„
    let tokenizer_path = r"C:\Users\chen-\Downloads\tokenizer.json";
    let weight_path = r"C:\Users\chen-\Downloads\model.safetensors";

    let tokenizer = LlamaTokenizer::from_file(tokenizer_path)?;

    if std::path::Path::new(weight_path).exists() {
        println!("ğŸ“¦ Loading weights from: {}", weight_path);
        ModelLoader::load_llama_weights(weight_path, &model.named_parameters())?;
    } else {
        println!("âš ï¸ Weights not found at {}, output will be random noise.", weight_path);
    }

    println!("\nâœ¨ System Ready. Type 'exit' to quit.");

    // Stop tokens
    let stop_ids: Vec<usize> = [
        tokenizer.token_to_id("</s>"),
        tokenizer.token_to_id("<|system|>"),
        tokenizer.token_to_id("<|user|>"),
        tokenizer.token_to_id("<|assistant|>"),
    ]
    .into_iter()
    .flatten()
    .collect();

    loop {
        print!("\nğŸ‘¤ User: ");
        io::stdout().flush()?;

        let mut input = String::new();
        io::stdin().read_line(&mut input)?;
        let user_msg = input.trim();

        if user_msg == "exit" || user_msg == "quit" {
            break;
        }
        if user_msg.is_empty() {
            continue;
        }

        print!("ğŸ¤– Assistant: ");
        io::stdout().flush()?;

        no_grad(|| {
            // 1) æ„å»º Prompt & Tokenize
            let system = "You are a helpful AI assistant.";
            let chat_prompt = build_tinyllama_chat_prompt(system, user_msg);
            let mut tokens = tokenizer.encode(&chat_prompt, false);
            let prompt_len = tokens.len();

            // 2) åˆå§‹åŒ– KV Cacheï¼ˆâœ… æ–° llama.rs APIï¼‰
            //   - ä¸å†ä½¿ç”¨ LlamaKVCache
            //   - ç”± model ç»Ÿä¸€åˆå§‹åŒ–ï¼Œå¹¶åœ¨æ¯è½®å¯¹è¯å‰ reset
            let mut kv_caches = model.init_kv_caches(1);
            model.reset_kv_caches(&mut kv_caches);

            let max_gen = 200;
            let mut prev_gen_text = String::new();

            // é‡‡æ ·å‚æ•°
            let temperature = 0.8;
            let top_p = 0.9;
            let repetition_penalty = 1.05;
            let recent_window = 64usize;

            // ä½ç½®æŒ‡é’ˆ posï¼ˆæ–°å®ç°é‡Œ pos è¢«å¿½ç•¥ï¼Œä½†ä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç ï¼‰
            let mut pos = 0usize;

            for _ in 0..max_gen {
                // 3) Prefill vs Decoding
                let input_ids: Vec<usize> = if pos == 0 {
                    tokens.clone()
                } else {
                    vec![*tokens.last().unwrap()]
                };

                let input_tensor = Tensor::from_array_no_grad(
                    Array::from_shape_vec(
                        (1, input_ids.len()),
                        input_ids.iter().map(|&x| x as f32).collect(),
                    )
                    .unwrap()
                    .into_dyn(),
                );

                // 4) Forward (ä¼ é€’ &mut caches å’Œ pos)
                // logits: [Batch=1, Seq, Vocab]
                let logits = model.forward(input_tensor, &mut kv_caches, pos);

                // æ›´æ–° posï¼ˆè™½ç„¶æ–°å®ç°å¿½ç•¥ posï¼Œä½†ç•™ç€æ— å®³ï¼‰
                pos += input_ids.len();

                // 5) å–æœ€åä¸€æ­¥ logits + é‡‡æ ·
                // è¿™é‡Œä¿æŒä½ åŸæ¥çš„å†™æ³•ï¼ˆä¸ä¼šè§¦å‘ E0716ï¼‰
                let logits_ref = logits.data_ref();
                let last_step_logits = logits_ref.slice(ndarray::s![0, -1, ..]);
                let logits_vec: Vec<f32> = last_step_logits.iter().copied().collect();

                let start = tokens.len().saturating_sub(recent_window);
                let recent = &tokens[start..];

                let next_token = sample_top_p(
                    &logits_vec,
                    temperature,
                    top_p,
                    repetition_penalty,
                    recent,
                );

                if stop_ids.contains(&next_token) {
                    break;
                }

                tokens.push(next_token);

                // 6) æµå¼è¾“å‡º
                let gen_tokens = &tokens[prompt_len..];
                let cur_gen_text = tokenizer.decode(gen_tokens, true);

                if cur_gen_text.contains("<|user|>") || cur_gen_text.contains("<|assistant|>") {
                    break;
                }
                print_new_suffix(&mut prev_gen_text, cur_gen_text);
            }

            println!();
        });
    }

    Ok(())
}
